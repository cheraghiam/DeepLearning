{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1c29e5",
   "metadata": {},
   "source": [
    "# Classification Metrics & Handling Imbalanced Data\n",
    "\n",
    "A practical guide to evaluation metrics and techniques for working with imbalanced datasets in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Core Evaluation Metrics\n",
    "\n",
    "## 1.1 Confusion Matrix\n",
    "\n",
    "The foundation of most classification metrics.\n",
    "\n",
    "|                      | Predicted Positive | Predicted Negative |\n",
    "|----------------------|-------------------|-------------------|\n",
    "| **Actual Positive**  | TP                | FN                |\n",
    "| **Actual Negative**  | FP                | TN                |\n",
    "\n",
    "- **TP** = True Positive  \n",
    "- **TN** = True Negative  \n",
    "- **FP** = False Positive (Type I error)  \n",
    "- **FN** = False Negative (Type II error)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Accuracy\n",
    "\n",
    "```math\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "```\n",
    "\n",
    "→ **Very misleading when classes are imbalanced.**\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Precision (Positive Predictive Value)\n",
    "\n",
    "```math\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "```\n",
    "\n",
    "→ Of all instances predicted as positive, how many were actually positive?  \n",
    "→ Important when **false positives are expensive** (spam detection, fraud alerts).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Recall (Sensitivity, True Positive Rate)\n",
    "\n",
    "```math\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "```\n",
    "\n",
    "→ Of all actual positive instances, how many did we correctly identify?  \n",
    "→ Critical when **false negatives are expensive** (cancer detection, defect detection).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 F1 Score\n",
    "\n",
    "Harmonic mean of Precision and Recall:\n",
    "\n",
    "```math\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "```\n",
    "\n",
    "→ Best single metric when you want balance between Precision and Recall.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Fβ Score\n",
    "\n",
    "Generalized form — gives more weight to recall or precision:\n",
    "\n",
    "```math\n",
    "F_\\beta = (1 + \\beta^2) \\times \n",
    "\\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall}\n",
    "```\n",
    "\n",
    "- β > 1 → favors recall (e.g., F2)  \n",
    "- β < 1 → favors precision (e.g., F0.5)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "Very robust metric — especially good for imbalanced data:\n",
    "\n",
    "```math\n",
    "MCC = \\frac{TP \\times TN - FP \\times FN}\n",
    "{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "```\n",
    "\n",
    "Range: **−1 to +1**  \n",
    "- +1 → perfect classifier  \n",
    "- 0 → random  \n",
    "- −1 → total disagreement  \n",
    "\n",
    "---\n",
    "\n",
    "## 1.8 Balanced Accuracy\n",
    "\n",
    "```math\n",
    "Balanced\\ Accuracy = \\frac{Recall + Specificity}{2}\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "```math\n",
    "Specificity = \\frac{TN}{TN + FP}\n",
    "```\n",
    "\n",
    "→ Much better than regular accuracy for imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9 ROC-AUC vs PR-AUC\n",
    "\n",
    "| Metric  | When to Prefer | Sensitive to Imbalance? | Typical Use Case |\n",
    "|----------|---------------|--------------------------|------------------|\n",
    "| ROC-AUC  | Balanced to moderately imbalanced | Moderately | General model comparison |\n",
    "| PR-AUC   | Highly imbalanced (<5–10%) | No | Rare event detection (fraud, defects) |\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Handling Imbalanced Datasets\n",
    "\n",
    "## 2.1 Resampling Techniques\n",
    "\n",
    "| Method | Type | Pros | Cons | Library Example |\n",
    "|--------|------|------|------|----------------|\n",
    "| Random Under-sampling | Under | Fast, simple | Loss of information | `imblearn.under_sampling` |\n",
    "| NearMiss | Under | Smarter selection | Can discard useful samples | `imblearn` |\n",
    "| Random Over-sampling | Over | No information loss | Risk of overfitting | `imblearn.over_sampling` |\n",
    "| SMOTE | Over (synthetic) | Creates realistic samples | Can create noisy samples | `imblearn.over_sampling.SMOTE` |\n",
    "| ADASYN | Over (synthetic) | Focuses on difficult examples | More complex | `imblearn` |\n",
    "| SMOTE + Tomek / ENN | Hybrid | Cleans boundary after synthesis | Computationally heavier | `imblearn.combine` |\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Algorithm-Level Solutions\n",
    "\n",
    "- **Class weights**  \n",
    "  Most sklearn classifiers support:  \n",
    "  ```python\n",
    "  class_weight='balanced'\n",
    "  ```\n",
    "  or a manual dictionary.\n",
    "\n",
    "- **Cost-sensitive learning**  \n",
    "  Define different misclassification costs.\n",
    "\n",
    "- **Ensemble methods designed for imbalance**\n",
    "  - BalancedRandomForestClassifier\n",
    "  - RUSBoost\n",
    "  - EasyEnsemble\n",
    "  - BalancedBagging\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Extremely Rare Positive Class (<1–2%)\n",
    "\n",
    "Consider treating the problem as **anomaly detection**:\n",
    "\n",
    "- Isolation Forest  \n",
    "- One-Class SVM  \n",
    "- Autoencoders (reconstruction error)  \n",
    "- Local Outlier Factor (LOF)  \n",
    "\n",
    "---\n",
    "\n",
    "# 3. Quick Reference – Which Metric to Use?\n",
    "\n",
    "| Situation | Recommended Metric(s) | Why? |\n",
    "|------------|----------------------|------|\n",
    "| Roughly balanced classes | Accuracy + F1 + ROC-AUC | All are reasonable |\n",
    "| Moderate imbalance (~5–30%) | F1 + Balanced Accuracy + MCC | Better reflect minority performance |\n",
    "| Severe imbalance (<5%) | PR-AUC + MCC + F2 | ROC-AUC becomes overly optimistic |\n",
    "| False negatives very costly | Recall, F2 score | Prioritize catching positives |\n",
    "| False positives very costly | Precision, F0.5 score | Avoid false alarms |\n",
    "| Need one interpretable number | MCC | Most balanced single-number metric |\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Final Practical Tips\n",
    "\n",
    "- Always stratify when splitting data:\n",
    "  ```python\n",
    "  train_test_split(X, y, stratify=y)\n",
    "  ```\n",
    "\n",
    "- Use appropriate scoring in cross-validation:\n",
    "  ```python\n",
    "  cross_val_score(model, X, y, scoring='f1')\n",
    "  cross_val_score(model, X, y, scoring='roc_auc')\n",
    "  cross_val_score(model, X, y, scoring='average_precision')\n",
    "  cross_val_score(model, X, y, scoring='matthews_corrcoef')\n",
    "  ```\n",
    "\n",
    "- Plot **both ROC and Precision-Recall curves**\n",
    "- Tune probability threshold after training (not always 0.5!)\n",
    "- When in doubt → report multiple metrics (Precision, Recall, F1, MCC, PR-AUC)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
